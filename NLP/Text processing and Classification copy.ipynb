{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVEEazxFZgr2",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-the-data\" data-toc-modified-id=\"1.-Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Load the data</a></span></li><li><span><a href=\"#2.-Filtering-out-the-noise\" data-toc-modified-id=\"2.-Filtering-out-the-noise-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Filtering out the noise</a></span></li><li><span><a href=\"#3.-Even-better-filtering\" data-toc-modified-id=\"3.-Even-better-filtering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Even better filtering</a></span></li><li><span><a href=\"#4.-Term-frequency-times-inverse-document-frequency\" data-toc-modified-id=\"4.-Term-frequency-times-inverse-document-frequency-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. Term frequency times inverse document frequency</a></span></li><li><span><a href=\"#5.-Utility-function\" data-toc-modified-id=\"5.-Utility-function-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. Utility function</a></span></li></ul></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjpQefesZgr4"
   },
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
    "\n",
    "License: CC-BY-SA-NC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1VXs8S8Zgr7"
   },
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Text data pre-processing</div>\n",
    "\n",
    "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers.\n",
    "\n",
    "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
    "\n",
    "Here are some pre-processing steps that can be performed on text:\n",
    "\n",
    "1. loading the data, removing attachements, merging title and body;\n",
    "2. tokenizing - splitting the text into atomic \"words\";\n",
    "3. removal of stop-words - very common words;\n",
    "4. removal of non-words - punctuation, numbers, gibberish;\n",
    "5. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
    "\n",
    "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGMK6qekZgr9"
   },
   "source": [
    "# 1. Text classification in English\n",
    "\n",
    "## 1.1 Load the data\n",
    "\n",
    "Let's first load the emails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26qQSlBNZpZY"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/SupaeroDataScience/deep-learning\n",
    "#!mv deep-learning/data .\n",
    "#!mv deep-learning/NLP/datasets .\n",
    "#!pip install nltk unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eYwNdKZiZgr9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails 2893\n",
      "email file: ../data/lingspam_public/bare/part3/6-232msg1.txt\n",
      "email is a spam: False\n",
      "Subject: foreign language in commercials\n",
      "\n",
      "content - length : 1937 greetings ! i ' m wondering if someone out there can identify the languages used in two recent ibm commercials . they have out a series of three ads with people in different countries talking about ibm equipment and services . one ad has two old gentlemen walking along what looks like the seine , speaking french . a second ad has two men sitting in what is apparently a middle eastern marketplace , and a third ad has nuns discussing ibm equipment on their way to mass . i assume the men are speaking arabic ( though i would be grateful to have that confirmed ) , but i have no idea what the nuns are speaking . anyone know ? i would also like to take this opportunity to thank all those who responded to my questions early last year about some french , italian and swedish expressions in some print ads . i meant to send individual thanks , but i lost the file in which i had the respondents ! please forgive me , and accept this general acknowledgment instead . if any of you who did respond would like to know more about what we did with your input , i would be glad to send you more information ( even the paper we wrote , if you want ) . thanks very much , mary ellen renryder @ idbsu . idbsu . edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_switch = 1\n",
    "if data_switch == 0:\n",
    "    train_dir = \"../data/ling-spam/train-mails/\"\n",
    "    email_path = [os.path.join(train_dir, f) for f in os.listdir(train_dir)]\n",
    "else:\n",
    "    train_dir = \"../data/lingspam_public/bare/\"\n",
    "    email_path = []\n",
    "    email_label = []\n",
    "    for d in os.listdir(train_dir):\n",
    "        folder = os.path.join(train_dir, d)\n",
    "        email_path += [os.path.join(folder, f) for f in os.listdir(folder)]\n",
    "        email_label += [f[0:3] == \"spm\" for f in os.listdir(folder)]\n",
    "print(\"number of emails\", len(email_path))\n",
    "email_nb = 8  # try 8 for a spam example\n",
    "print(\"email file:\", email_path[email_nb])\n",
    "print(\"email is a spam:\", email_label[email_nb])\n",
    "print(open(email_path[email_nb]).read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBPuFTw2ZgsA"
   },
   "source": [
    "## 1.2. Filtering out the noise\n",
    "\n",
    "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- To remove stop-words, we set: `stop_words='english'`\n",
    "- To convert all words to lowercase: `lowercase=True`\n",
    "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UKLxbfC-ZgsB"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvect = CountVectorizer(input=\"filename\", stop_words=\"english\", lowercase=True)\n",
    "word_count = countvect.fit_transform(email_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8eLR5hi3ZgsC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 60618\n",
      "Document - words matrix: (2893, 60618)\n",
      "First words: ['00', '000', '0000', '00001', '00003000140', '00003003958', '00007', '0001', '00010', '00014', '0003', '00036', '000bp', '000s', '000yen', '001', '0010', '0010010034', '0011', '00133', '0014', '00170', '0019', '00198', '002', '002656', '0027', '003', '0030', '0031', '00333', '0037', '0039', '003n7', '004', '0041', '0044', '0049', '005', '0057', '006', '0067', '007', '00710', '0073', '0074', '00799', '008', '009', '00919680', '0094', '00a', '00am', '00arrival', '00b', '00coffee', '00congress', '00d', '00dinner', '00f', '00h', '00hfstahlke', '00i', '00j', '00l', '00m', '00p', '00pm', '00r', '00t', '00tea', '00the', '00uzheb', '01', '0100', '01003', '01006', '0104', '0106', '01075', '0108', '011', '0111', '0117', '0118', '01202', '01222', '01223', '01225', '01232', '01235', '01273', '013', '0131', '01334', '0135', '01364', '0139', '013953', '013a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/jupyter-arm-pytorch/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7H9GRQxXZgsC"
   },
   "source": [
    "## 1.3. Even better filtering\n",
    "\n",
    "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
    "\n",
    "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
    "\n",
    "- tokenizes;\n",
    "- removes punctuation;\n",
    "- removes stop-words;\n",
    "- removes non-English and misspelled words (optional);\n",
    "- removes 1-character words;\n",
    "- removes non-alphabetical words (numbers and codes essentially).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qbwk9Mp0ZgsD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/arthur/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/arthur/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/arthur/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/arthur/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OLBm1kLIZgsE"
   },
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if self.remove_non_words:\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word) > 1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXUNGD9AZgsG"
   },
   "source": [
    "The LemmaTokenizer defined above will be applied further in this example. The next step is to define the Count Vectorization pipeline using this Tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sa66gFMlZgsG"
   },
   "outputs": [],
   "source": [
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
    "bow = countvect.fit_transform(email_path)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IIKuiDWSZgsH",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 14279\n",
      "Document - words matrix: (2893, 14279)\n",
      "First words: ['aa', 'aal', 'aba', 'aback', 'abacus', 'abandon', 'abandoned', 'abandonment', 'abbas', 'abbreviation', 'abdomen', 'abduction', 'abed', 'aberrant', 'aberration', 'abide', 'abiding', 'abigail', 'ability', 'ablative', 'ablaut', 'able', 'abler', 'aboard', 'abolition', 'abord', 'aboriginal', 'aborigine', 'abound', 'abox', 'abreast', 'abridged', 'abroad', 'abrogate', 'abrook', 'abruptly', 'abscissa', 'absence', 'absent', 'absolute', 'absolutely', 'absoluteness', 'absolutist', 'absolutive', 'absolutization', 'absorbed', 'absorption', 'abstract', 'abstraction', 'abstractly', 'abstractness', 'absurd', 'absurdity', 'abu', 'abundance', 'abundant', 'abuse', 'abusive', 'abyss', 'academe', 'academic', 'academically', 'academician', 'academy', 'accelerate', 'accelerated', 'accelerative', 'accent', 'accentuate', 'accentuation', 'accept', 'acceptability', 'acceptable', 'acceptance', 'acceptation', 'accepted', 'acception', 'access', 'accessibility', 'accessible', 'accessibly', 'accidence', 'accident', 'accidental', 'accidentality', 'accidentally', 'acclaim', 'accommodate', 'accommodation', 'accompany', 'accomplish', 'accomplished', 'accomplishment', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountability', 'accountant']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", bow.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb-3Z74eZgsI"
   },
   "source": [
    "## 1.4. Using the bag of words (BOW) object to classify spam\n",
    "\n",
    "Let's start by splitting the data into train and test sets, using 20% of the data for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Gf-54hwSZgsI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow, email_label, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rS7EDB-ZgsI"
   },
   "source": [
    "In this simple example we will use a Logistic Regression Classifier. Let's fit it to our Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BxD1T5OiZgsJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9948186528497409\n",
      "Precision : 1.0\n",
      "Recall : 0.970873786407767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\", metrics.accuracy_score(y_test, y_predicted))\n",
    "print(\"Precision :\", metrics.precision_score(y_test, y_predicted))\n",
    "print(\"Recall :\", metrics.recall_score(y_test, y_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKsqSjg0ZgsJ"
   },
   "source": [
    "In many cases, Bag of Words can provide sufficient information for classification. In this case, the accuracy reached by our classifier is pretty good.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naewZmg0ZgsK"
   },
   "source": [
    "## 1.5. Term frequency times inverse document frequency\n",
    "\n",
    "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "s9imM6DqZgsK",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email:\n",
      "Subject: job posting - apple-iss research center\n",
      "\n",
      "content - length : 3386 apple-iss research center a us $ 10 million joint venture between apple computer inc . and the institute of systems science of the national university of singapore , located in singapore , is looking for : a senior speech scientist - - - - - - - - - - - - - - - - - - - - - - - - - the successful candidate will have research expertise in computational linguistics , including natural language processing and * * english * * and * * chinese * * statistical language modeling . knowledge of state-of - the-art corpus-based n - gram language models , cache language models , and part-of - speech language models are required . a text - to - speech project leader - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - the successful candidate will have research expertise expertise in two or more of the following areas : computational linguistics , including natural language parsing , lexical database design , and statistical language modeling ; text tokenization and normalization ; prosodic analysis . substantial knowledge of the phonology , syntax , and semantics of chinese is required . knowledge of acoustic phonetics and / or speech signal processing is desirable . both candidates will have a phd with at least 2 to 4 years of relevant work experience , or a technical msc degree with at least 5 to 7 years of experienc e . very strong software engineering skills , including design and implementation , and productization are required in these positions . knowledge of c , c + + and unix are preferred . a unix & c programmer - - - - - - - - - - - - - - - - - - - - we are looking for an experienced unix & c programmer , preferably with good industry experience , to join us in breaking new frontiers . strong knowledge of unix tools ( compilers , linkers , make , x - windows , e - mac , . . . ) and experience in matlab required . sun and silicon graphic experience is an advantage . programmers with less than two years industry experience need not apply . these positions include interaction with scientists in the national university of singapore , and with apple 's speech research and productization efforts located in cupertino , california . attendance and publication in international scientific / engineering conferences is encouraged . benefits include an internationally competitive salary , housing subsidy , and relocation expenses . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ send a complete resume , enclosing personal particulars , qualifications , experience and contact telephone number to : mr jean - luc lebrun center manager apple - iss research center , institute of systems science heng mui keng terrace , singapore 0511 tel : ( 65 ) 772-6571 fax : ( 65 ) 776-4005 email : jllebrun @ iss . nus . sg\n",
      "\n",
      "Bag of words representation (104 words in dict):\n",
      "{'subject': 1, 'job': 1, 'posting': 1, 'apple': 5, 'research': 6, 'center': 4, 'content': 1, 'length': 1, 'u': 2, 'million': 1, 'joint': 1, 'venture': 1, 'computer': 1, 'institute': 2, 'science': 2, 'national': 2, 'university': 2, 'looking': 2, 'senior': 1, 'speech': 5, 'scientist': 1, 'successful': 2, 'candidate': 2, 'computational': 2, 'linguistics': 2, 'natural': 2, 'language': 7, 'statistical': 2, 'modeling': 2, 'knowledge': 5, 'state': 1, 'art': 1, 'corpus': 1, 'based': 1, 'gram': 1, 'cache': 1, 'part': 1, 'text': 2, 'project': 1, 'leader': 1, 'two': 2, 'following': 1, 'lexical': 1, 'design': 2, 'normalization': 1, 'prosodic': 1, 'analysis': 1, 'substantial': 1, 'phonology': 1, 'syntax': 1, 'semantics': 1, 'acoustic': 1, 'phonetics': 1, 'signal': 1, 'desirable': 1, 'least': 2, 'relevant': 1, 'work': 1, 'experience': 6, 'technical': 1, 'degree': 1, 'strong': 2, 'engineering': 2, 'implementation': 1, 'preferred': 1, 'programmer': 2, 'experienced': 1, 'preferably': 1, 'good': 1, 'industry': 2, 'join': 1, 'breaking': 1, 'new': 1, 'make': 1, 'mac': 1, 'sun': 1, 'silicon': 1, 'graphic': 1, 'advantage': 1, 'le': 1, 'need': 1, 'apply': 1, 'include': 2, 'interaction': 1, 'attendance': 1, 'publication': 1, 'international': 1, 'scientific': 1, 'internationally': 1, 'competitive': 1, 'salary': 1, 'housing': 1, 'subsidy': 1, 'relocation': 1, 'send': 1, 'complete': 1, 'resume': 1, 'personal': 1, 'contact': 1, 'telephone': 1, 'number': 1, 'jean': 1, 'manager': 1, 'terrace': 1}\n",
      "\n",
      "Vector reprensentation (104 non-zero elements):\n",
      "  (0, 12153)\t1\n",
      "  (0, 6803)\t1\n",
      "  (0, 9514)\t1\n",
      "  (0, 651)\t5\n",
      "  (0, 10559)\t6\n",
      "  (0, 1925)\t4\n",
      "  (0, 2680)\t1\n",
      "  (0, 7148)\t1\n",
      "  (0, 13190)\t2\n",
      "  (0, 7859)\t1\n",
      "  (0, 6810)\t1\n",
      "  (0, 13708)\t1\n",
      "  (0, 2475)\t1\n",
      "  (0, 6485)\t2\n",
      "  (0, 11085)\t2\n",
      "  (0, 8203)\t2\n",
      "  (0, 13427)\t2\n",
      "  (0, 7371)\t2\n",
      "  (0, 11223)\t1\n",
      "  (0, 11809)\t5\n",
      "  (0, 11088)\t1\n",
      "  (0, 12220)\t2\n",
      "  (0, 1762)\t2\n",
      "  (0, 2473)\t2\n",
      "  (0, 7261)\t2\n",
      "  :\t:\n",
      "  (0, 7083)\t1\n",
      "  (0, 8251)\t1\n",
      "  (0, 660)\t1\n",
      "  (0, 6244)\t2\n",
      "  (0, 6545)\t1\n",
      "  (0, 892)\t1\n",
      "  (0, 9936)\t1\n",
      "  (0, 6589)\t1\n",
      "  (0, 11086)\t1\n",
      "  (0, 6591)\t1\n",
      "  (0, 2418)\t1\n",
      "  (0, 10948)\t1\n",
      "  (0, 5966)\t1\n",
      "  (0, 12189)\t1\n",
      "  (0, 10449)\t1\n",
      "  (0, 11220)\t1\n",
      "  (0, 2430)\t1\n",
      "  (0, 10637)\t1\n",
      "  (0, 9123)\t1\n",
      "  (0, 2672)\t1\n",
      "  (0, 12614)\t1\n",
      "  (0, 8468)\t1\n",
      "  (0, 6783)\t1\n",
      "  (0, 7524)\t1\n",
      "  (0, 12675)\t1\n"
     ]
    }
   ],
   "source": [
    "mail_number = 0\n",
    "text = open(email_path[mail_number]).read()\n",
    "print(\"Original email:\")\n",
    "print(text)\n",
    "\n",
    "emailBagOfWords = {\n",
    "    feat2word[i]: bow[mail_number, i] for i in bow[mail_number, :].nonzero()[1]\n",
    "}\n",
    "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep=\"\")\n",
    "print(emailBagOfWords)\n",
    "print(\n",
    "    \"\\nVector reprensentation (\",\n",
    "    bow[mail_number, :].nonzero()[1].shape[0],\n",
    "    \" non-zero elements):\",\n",
    "    sep=\"\",\n",
    ")\n",
    "print(bow[mail_number, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCV1BC1RZgsL"
   },
   "source": [
    "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pNuCQwo8ZgsL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 14279)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer().fit_transform(bow)\n",
    "tfidf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhKhbIoeZgsM"
   },
   "source": [
    "Let's run the classification process again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-NkGrarpZgsM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9671848013816926\n",
      "Precision : 1.0\n",
      "Recall : 0.8137254901960784\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, email_label, test_size=0.2)\n",
    "\n",
    "# Fitting classifier\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Testing classifier\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\", metrics.accuracy_score(y_test, y_predicted))\n",
    "print(\"Precision :\", metrics.precision_score(y_test, y_predicted))\n",
    "print(\"Recall :\", metrics.recall_score(y_test, y_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_icBmOuZgsM"
   },
   "source": [
    "In this simplae case, additional filtering is unecessary and even removed some information. There is indeed likely a link between the abundance of words/long emails and the fact that this email is a spam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcZVAEwsZgsN"
   },
   "source": [
    "# 2. Text classification in French\n",
    "\n",
    "The previously used dataset is a widely used dataset for introductory text classification.\n",
    "\n",
    "The field of Natural Language Understanding, and Natural Language Classification in particular, suffers from two challenges :\n",
    "\n",
    "- Adapting the features and methodologies to various and more complex datasets\n",
    "- Adapting the process to languages other than english\n",
    "\n",
    "Concerning the latter, one has to take into account that most of NLU research is currently performed on english. Datasets are rarely available for other languages, and the algorithms proposed for better NLU are often left untested on foreign data.\n",
    "French, for instance, has less efficient lemmatization (french is a richly flected language). In the following section, we will reuse the same methodologies on a french dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4TzD5FKrZgsN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>game_fr</th>\n",
       "      <th>game_en</th>\n",
       "      <th>platform</th>\n",
       "      <th>website_rating</th>\n",
       "      <th>public_rating</th>\n",
       "      <th>publishor/developer</th>\n",
       "      <th>release</th>\n",
       "      <th>type</th>\n",
       "      <th>classification</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>.hack//G.U. Last Recode</td>\n",
       "      <td>.hack//G.U. Last Recode</td>\n",
       "      <td>PS4</td>\n",
       "      <td>14/20</td>\n",
       "      <td>--/20</td>\n",
       "      <td>Bandai Namco Entertainment</td>\n",
       "      <td>03 Novembre 2017</td>\n",
       "      <td>RPG</td>\n",
       "      <td>+12 ans</td>\n",
       "      <td>http://www.jeuxvideo.com/jeux/ps4/jeu-674262/</td>\n",
       "      <td>Au contraire d’autres titres, None ,''.hack'' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>.hack//G.U. Vol.1//Rebirth</td>\n",
       "      <td>.hack//G.U. Vol. 1//Rebirth</td>\n",
       "      <td>PS2</td>\n",
       "      <td>15/20</td>\n",
       "      <td>18.2/20</td>\n",
       "      <td>Bandai Namco CyberConnect2</td>\n",
       "      <td>Date de sortie inconnue</td>\n",
       "      <td>RPG</td>\n",
       "      <td>+7 ans</td>\n",
       "      <td>http://www.jeuxvideo.com/jeux/playstation-2-ps...</td>\n",
       "      <td>Avec plus de 20 œuvres de fiction sur de mult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>.hack//Infection Part 1</td>\n",
       "      <td>.hack//Infection: Part 1</td>\n",
       "      <td>PS2</td>\n",
       "      <td>15/20</td>\n",
       "      <td>15.1/20</td>\n",
       "      <td>CyberConnect2 Bandai</td>\n",
       "      <td>26 Mars 2004</td>\n",
       "      <td>RPG</td>\n",
       "      <td>+12 ans</td>\n",
       "      <td>http://www.jeuxvideo.com/jeux/playstation-2-ps...</td>\n",
       "      <td>S'appuyant sur la maxime « Mieux vaut tard qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>.hack//Mutation Part 2</td>\n",
       "      <td>.hack//Mutation: Part 2</td>\n",
       "      <td>PS2</td>\n",
       "      <td>14/20</td>\n",
       "      <td>16.4/20</td>\n",
       "      <td>Bandai CyberConnect2</td>\n",
       "      <td>18 Juin 2004</td>\n",
       "      <td>RPG</td>\n",
       "      <td>+12 ans</td>\n",
       "      <td>http://www.jeuxvideo.com/jeux/playstation-2-ps...</td>\n",
       "      <td>Voici enfin le second volet de la quadrilogie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>.hack//Outbreak Part 3</td>\n",
       "      <td>.hack//Outbreak: Part 3</td>\n",
       "      <td>PS2</td>\n",
       "      <td>13/20</td>\n",
       "      <td>15.3/20</td>\n",
       "      <td>CyberConnect2 Atari</td>\n",
       "      <td>17 Septembre 2004</td>\n",
       "      <td>RPG</td>\n",
       "      <td>+12 ans</td>\n",
       "      <td>http://www.jeuxvideo.com/jeux/playstation-2-ps...</td>\n",
       "      <td>Comme la maxime «Jamais deux sans trois» ne c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     game_fr                      game_en  \\\n",
       "0           1     .hack//G.U. Last Recode      .hack//G.U. Last Recode   \n",
       "1           2  .hack//G.U. Vol.1//Rebirth  .hack//G.U. Vol. 1//Rebirth   \n",
       "2           3     .hack//Infection Part 1     .hack//Infection: Part 1   \n",
       "3           4      .hack//Mutation Part 2      .hack//Mutation: Part 2   \n",
       "4           5      .hack//Outbreak Part 3      .hack//Outbreak: Part 3   \n",
       "\n",
       "  platform website_rating public_rating         publishor/developer  \\\n",
       "0      PS4          14/20         --/20  Bandai Namco Entertainment   \n",
       "1      PS2          15/20       18.2/20  Bandai Namco CyberConnect2   \n",
       "2      PS2          15/20       15.1/20        CyberConnect2 Bandai   \n",
       "3      PS2          14/20       16.4/20        Bandai CyberConnect2   \n",
       "4      PS2          13/20       15.3/20         CyberConnect2 Atari   \n",
       "\n",
       "                   release type classification  \\\n",
       "0         03 Novembre 2017  RPG        +12 ans   \n",
       "1  Date de sortie inconnue  RPG         +7 ans   \n",
       "2             26 Mars 2004  RPG        +12 ans   \n",
       "3             18 Juin 2004  RPG        +12 ans   \n",
       "4        17 Septembre 2004  RPG        +12 ans   \n",
       "\n",
       "                                                 url  \\\n",
       "0      http://www.jeuxvideo.com/jeux/ps4/jeu-674262/   \n",
       "1  http://www.jeuxvideo.com/jeux/playstation-2-ps...   \n",
       "2  http://www.jeuxvideo.com/jeux/playstation-2-ps...   \n",
       "3  http://www.jeuxvideo.com/jeux/playstation-2-ps...   \n",
       "4  http://www.jeuxvideo.com/jeux/playstation-2-ps...   \n",
       "\n",
       "                                         description  \n",
       "0  Au contraire d’autres titres, None ,''.hack'' ...  \n",
       "1   Avec plus de 20 œuvres de fiction sur de mult...  \n",
       "2   S'appuyant sur la maxime « Mieux vaut tard qu...  \n",
       "3   Voici enfin le second volet de la quadrilogie...  \n",
       "4   Comme la maxime «Jamais deux sans trois» ne c...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load video games reviews\n",
    "vgr = pd.read_csv(\"datasets/jvc.csv\")\n",
    "vgr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "17I9wrn8ZgsN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: ylabel='Frequency'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#convert rating to numeric values, and plot the histogram of values\n",
    "rating=vgr.website_rating.apply(lambda k: k[:-3])\n",
    "vgr['rating']=pd.to_numeric(rating)\n",
    "vgr.rating.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JB1HYixvZgsO"
   },
   "source": [
    "Most games seem to have a rating between 11 and 16. In this exercise, we will try to determine if we can determine if a game is very good (rating above 16) or very bad (rating below 11) based only on the summary of its review.\n",
    "\n",
    "Let's start by splitting the dataset between good and bad games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nZWhweYuZgsO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6r/vdmz5zjj6_d8769grj_ht__80000gn/T/ipykernel_29158/1438388647.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad['quality']=pd.Series([\"bad\"]*len(bad.index),index=bad.index)\n",
      "/var/folders/6r/vdmz5zjj6_d8769grj_ht__80000gn/T/ipykernel_29158/1438388647.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good['quality']=pd.Series([\"good\"]*len(good.index),index=good.index)\n"
     ]
    }
   ],
   "source": [
    "bad=vgr[(vgr.rating<=11) & (vgr.platform==\"PC\")]\n",
    "bad['quality']=pd.Series([\"bad\"]*len(bad.index),index=bad.index)\n",
    "good=vgr[(vgr.rating>=16) & (vgr.platform==\"PC\")]\n",
    "good['quality']=pd.Series([\"good\"]*len(good.index),index=good.index)\n",
    "selected_games=pd.concat([good,bad]).dropna()\n",
    "\n",
    "#Keep only reviews and \n",
    "game_reviews=selected_games['description']\n",
    "game_quality=selected_games['quality']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRuc6EaNZgsO"
   },
   "source": [
    "Lemmatization in French is a tricky issue.\n",
    "\n",
    "One example : the verb finir can be expressed as finissons, finirez, finisse, finit, etc...\n",
    "Lemmatization is typically less efficient in french than in english.\n",
    "\n",
    "Another alternative is to use Stemming instead. Stemming uses RegEx rules to truncate the end of a word that would normally correspond to conjugations, inflections, etc...\n",
    "Stemming destructs the readability of the words by truncating their end, but runs faster than Lemmatization\n",
    "\n",
    "In the next cell, we adapt the LemmaTokenizer that we defined earlier using a FrenchStemmer instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vA9TEMYXZgsO"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "class FrenchStemTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.st = FrenchStemmer()\n",
    "        self.stopwords = set(stopwords.words(\"french\"))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if self.remove_non_words:\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word) > 1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.st.stem(t) for t in word_list]\n",
    "\n",
    "\n",
    "countvect = CountVectorizer(tokenizer=FrenchStemTokenizer(remove_non_words=True))\n",
    "bow_games = countvect.fit_transform(game_reviews)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eirJF0KjZgsP"
   },
   "source": [
    "### Classify with BOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lbgEZqdpZgsP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2349\n",
      "Number of words: 3161\n",
      "Document - words matrix: (2349, 3161)\n",
      "First words: ['abandon', 'abomin', 'abord', 'abras', 'absenc', 'absolut', 'absorb', 'abstract', 'abyssal', 'academy', 'accent', 'accept', 'access', 'accessibl', 'acclaim', 'accord', 'accouch', 'accru', 'accus', 'ace', 'aci', 'acolyt', 'acquisit', 'acquit', 'acquitt', 'act', 'action', 'activ', 'activity', 'actual', 'ad', 'adag', 'adapt', 'add', 'addict', 'addit', 'adieu', 'adieux', 'admir', 'adolescent', 'adopt', 'ador', 'adrenalin', 'adroit', 'advanc', 'advanced', 'adventur', 'advers', 'aero', 'affect', 'affili', 'affirm', 'affluenc', 'afflux', 'affront', 'afraid', 'after', 'afterbirth', 'against', 'age', 'agend', 'agent', 'aggress', 'agit', 'agon', 'agricol', 'agricultur', 'ah', 'aid', 'aiguill', 'aim', 'aion', 'air', 'al', 'alan', 'album', 'alfa', 'ali', 'alien', 'align', 'aliment', 'aliv', 'all', 'allan', 'allemand', 'aller', 'allianc', 'allur', 'allus', 'almost', 'alon', 'alpha', 'alphabet', 'altern', 'am', 'amass', 'amateur', 'amatric', 'ambit', 'ambivalent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/jupyter-arm-pytorch/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(game_reviews))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", bow_games.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "54z4k0VpZgsP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6617021276595745\n",
      "Precision : 0.6628352490421456\n",
      "Recall : 0.7090163934426229\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    bow_games, game_quality, test_size=0.2\n",
    ")\n",
    "\n",
    "# Fitting classifier\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Testing classifier\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\", metrics.accuracy_score(y_test, y_predicted))\n",
    "print(\"Precision :\", metrics.precision_score(y_test, y_predicted, pos_label=\"good\"))\n",
    "print(\"Recall :\", metrics.recall_score(y_test, y_predicted, pos_label=\"good\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLqa6tFTZgsQ"
   },
   "source": [
    "### Classify using tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hoDUgqKgZgsQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2349, 3161)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_games = TfidfTransformer().fit_transform(bow_games)\n",
    "tfidf_games.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bLCm-xlzZgsQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6787234042553192\n",
      "Precision : 0.6828358208955224\n",
      "Recall : 0.7349397590361446\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_games, game_quality, test_size=0.2\n",
    ")\n",
    "\n",
    "# Fitting classifier\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Testing classifier\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\", metrics.accuracy_score(y_test, y_predicted))\n",
    "print(\"Precision :\", metrics.precision_score(y_test, y_predicted, pos_label=\"good\"))\n",
    "print(\"Recall :\", metrics.recall_score(y_test, y_predicted, pos_label=\"good\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYmgT33VZgsR"
   },
   "source": [
    "## Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up7NxbxnZgsR"
   },
   "source": [
    "Tfidf and Bow are usually very efficient features to manipulate during cassification. However, please note that their size is directly related to the size of the vocabulary of our corpus. \n",
    "\n",
    "In our current example, even when using stemming, the dimensionnality of our bow or tfidf vectors is still very high (3161). This is not maintaintable with increasing corpora sizes.\n",
    "\n",
    "In this sction, we will use the word2vec embeddings, that address this issue by proposing an architecture that learns individual representations for words in a vector space of given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "L38iGShaZgsS"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "import unidecode\n",
    "\n",
    "class FrenchTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('french'))\n",
    "        self.words = set(words.words())\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [unidecode.unidecode(t) for t in word_list]\n",
    "\n",
    "tok=FrenchTokenizer()\n",
    "\n",
    "text_for_word2vec=[tok(sent) for sent in game_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRaGsoecZgsS"
   },
   "source": [
    "The operation above will tokenize all texts by keeping stemmed tokens. Please note the following choices :\n",
    "\n",
    "- we have applied stemming in order to reduce the dimensionality of our feature space\n",
    "- we have removed stop words, in order to not let context be learned with it. (depending on the use case, you may want to keep them or remove them)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOWsPngIZgsS"
   },
   "source": [
    "We can now train the Word2Vec model :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "XliGqhWMZgsT"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# model=Word2Vec(text_for_word2vec,size=200,window=5,min_count=1)\n",
    "# model=Word2Vec(text_for_word2vec,vector_size=200,window=5,min_count=1)\n",
    "model=Word2Vec(text_for_word2vec,window=5,min_count=1)\n",
    "# model=Word2Vec(text_for_word2vec, max_vocab_size=200,window=5,min_count=1)\n",
    "model.save(\"word2vec.model\")\n",
    "w2v=dict(zip(model.wv.index_to_key, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXaVjIVMZgsT"
   },
   "source": [
    "Let's check word similarity in our trained data :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "M0y6VW1dZgsT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('genre', 0.9999246597290039),\n",
       " ('titre', 0.9999223947525024),\n",
       " ('comme', 0.9999170899391174),\n",
       " ('ou', 0.999904215335846),\n",
       " ('donc', 0.9999004006385803),\n",
       " ('Le', 0.9998994469642639),\n",
       " ('plus', 0.9998963475227356),\n",
       " ('jeux', 0.9998959302902222),\n",
       " ('faire', 0.9998956918716431),\n",
       " ('peu', 0.9998952746391296)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"jeu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDKaY4dJZgsU"
   },
   "source": [
    "Let's now try again to classify our samples using these embddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ZVj4nK80ZgsU"
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self,word2vec,dim):\n",
    "        self.word2vec=word2vec\n",
    "        self.dim=dim\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "i_n-EJW5ZgsV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 &lt;__main__.MeanEmbeddingVectorizer object at 0x15ea13f40&gt;),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 &lt;__main__.MeanEmbeddingVectorizer object at 0x15ea13f40&gt;),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MeanEmbeddingVectorizer</label><div class=\"sk-toggleable__content\"><pre>&lt;__main__.MeanEmbeddingVectorizer object at 0x15ea13f40&gt;</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 <__main__.MeanEmbeddingVectorizer object at 0x15ea13f40>),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(game_reviews,game_quality,test_size=0.2)\n",
    "\n",
    "pipe=Pipeline([('vectorizer',MeanEmbeddingVectorizer(w2v,200)),('classifier',lr_classifier)])\n",
    "\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Tg9TYPRdZgsV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5425531914893617\n",
      "Precision : 0.5425531914893617\n",
      "Recall : 1.0\n"
     ]
    }
   ],
   "source": [
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,predicted,pos_label=\"good\"))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,predicted,pos_label=\"good\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNCJSWtaZgsW"
   },
   "source": [
    "What we observe here is that word2vec embeddings perform worse than what we learned from BOW or TFIDF.\n",
    "\n",
    "In our case, the training corpus for the embeddings was not large enough to ensure proper convergence and representation of the words.\n",
    "\n",
    "It is also common that for smaller corpora (<10.000 docs approximately), TFIDF usually performs better for classification, whereas Word2Vec produces better results with larger corpora and across domains (e.g. training on data from Wikipedia, and then using the vectors on data from another field)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('jupyter-arm-pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "6946b92c37b8ad9fada1ae6c49414674dbf58c9093c6278438c3f51701a14899"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
